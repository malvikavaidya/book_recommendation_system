{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import contractions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import string\n",
    "def clean_text(text):\n",
    "    # remove numbers\n",
    "    text_nonum = re.sub(r'\\d+', '', text)\n",
    "    # remove punctuations and convert characters to lower case\n",
    "    text_nopunct = \"\"\n",
    "    for char in text_nonum:\n",
    "        if char not in string.punctuation:\n",
    "            text_nopunct += char.lower()\n",
    "        else:    \n",
    "            text_nopunct += \" \"\n",
    "    # substitute multiple whitespace with single whitespace\n",
    "    # Also, removes leading and trailing whitespaces\n",
    "    text_no_doublespace = re.sub('\\s+', ' ', text_nopunct).strip()\n",
    "    return text_no_doublespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = []\n",
    "for description in book_descriptions:\n",
    "    descrption = contractions.fix(description)\n",
    "    description = clean_text(description)\n",
    "    tokens = word_tokenize(description)\n",
    "    tokenized.append(tokens)\n",
    "\n",
    "tokenized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered = []\n",
    "\n",
    "for word_tokens in tokenized:\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words and w != \"s\":\n",
    "            filtered_sentence.append(w)\n",
    "    filtered.append(filtered_sentence)\n",
    "\n",
    "filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemma_descriptions = []\n",
    "for description in filtered:\n",
    "    lemma = []\n",
    "    for w in description:\n",
    "        #this is so jank\n",
    "        #ideally we would have to do POS tagging and then pass the actual tag to the function\n",
    "        w = lemmatizer.lemmatize(w,  'v')\n",
    "        lemma.append(lemmatizer.lemmatize(w,  'n'))\n",
    "    lemma_descriptions.append(lemma)    \n",
    "\n",
    "lemma_descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_vectorize = []\n",
    "for description in lemma_descriptions:\n",
    "    book_descrip = \"\"\n",
    "    for w in description:\n",
    "        book_descrip += w + \" \"\n",
    "    text_to_vectorize.append(book_descrip)\n",
    "print(text_to_vectorize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(text_to_vectorize)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)\n",
    "df\n",
    "\n",
    "#slayyyyy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
